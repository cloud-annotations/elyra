{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [4]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZSL793i7KuM",
    "papermill": {
     "duration": 0.010158,
     "end_time": "2020-10-29T03:18:21.505686",
     "exception": false,
     "start_time": "2020-10-29T03:18:21.495528",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T03:18:21.527011Z",
     "iopub.status.busy": "2020-10-29T03:18:21.526164Z",
     "iopub.status.idle": "2020-10-29T03:18:21.528241Z",
     "shell.execute_reply": "2020-10-29T03:18:21.528713Z"
    },
    "id": "GmloKE6Qx8Lu",
    "papermill": {
     "duration": 0.016063,
     "end_time": "2020-10-29T03:18:21.529050",
     "exception": false,
     "start_time": "2020-10-29T03:18:21.512987",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "credentials = {\n",
    "    \"bucket\": \"BUCKET\",\n",
    "    \"access_key_id\": \"ACCESS_KEY_ID\",\n",
    "    \"secret_access_key\": \"SECRET_ACCESS_KEY\",\n",
    "    \"endpoint_url\": \"ENDPOINT_URL\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T03:18:21.552136Z",
     "iopub.status.busy": "2020-10-29T03:18:21.551368Z",
     "iopub.status.idle": "2020-10-29T03:18:21.554332Z",
     "shell.execute_reply": "2020-10-29T03:18:21.553842Z"
    },
    "id": "hVPzEKoLuEHy",
    "papermill": {
     "duration": 0.018118,
     "end_time": "2020-10-29T03:18:21.554540",
     "exception": false,
     "start_time": "2020-10-29T03:18:21.536422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_TRAIN_STEPS = 500\n",
    "\n",
    "import os\n",
    "CLOUD_ANNOTATIONS_MOUNT = os.path.join('content', credentials['bucket'])\n",
    "ANNOTATIONS_JSON_PATH   = os.path.join(CLOUD_ANNOTATIONS_MOUNT, '_annotations.json')\n",
    "\n",
    "DATA_PATH         = 'content/data'\n",
    "LABEL_MAP_PATH    = os.path.join(DATA_PATH, 'label_map.pbtxt')\n",
    "TRAIN_RECORD_PATH = os.path.join(DATA_PATH, 'train.record')\n",
    "VAL_RECORD_PATH   = os.path.join(DATA_PATH, 'val.record')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XINCKkPshgz",
    "papermill": {
     "duration": 0.006355,
     "end_time": "2020-10-29T03:18:21.567607",
     "exception": false,
     "start_time": "2020-10-29T03:18:21.561252",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Install the TensorFlow Object Detection API\n",
    "In order to use the TensorFlow Object Detection API, we need to clone it's GitHub Repo.\n",
    "\n",
    "### Dependencies\n",
    "Most of the dependencies required come preloaded in Google Colab. The only additional package we need to install is TensorFlow.js, which is used for converting our trained model to a model that is compatible for the web.\n",
    "\n",
    "### Protocol Buffers\n",
    "The TensorFlow Object Detection API relies on what are called `protocol buffers` (also known as `protobufs`). Protobufs are a language neutral way to describe information. That means you can write a protobuf once and then compile it to be used with other languages, like Python, Java or C.\n",
    "\n",
    "The `protoc` command used below is compiling all the protocol buffers in the `object_detection/protos` folder for Python.\n",
    "\n",
    "### Environment\n",
    "To use the object detection api we need to add it to our `PYTHONPATH` along with `slim` which contains code for training and evaluating several widely used Convolutional Neural Network (CNN) image classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T03:18:21.597569Z",
     "iopub.status.busy": "2020-10-29T03:18:21.596860Z",
     "iopub.status.idle": "2020-10-29T03:18:22.158218Z",
     "shell.execute_reply": "2020-10-29T03:18:22.158693Z"
    },
    "id": "o33_jgwGm3NV",
    "papermill": {
     "duration": 0.584316,
     "end_time": "2020-10-29T03:18:22.158906",
     "exception": false,
     "start_time": "2020-10-29T03:18:21.574590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: pip: command not found\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: pip: command not found\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: pip: command not found\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/niko/Documents/cloud-annotations-elyra/models/research\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "\n",
    "# Clone the tensorflow models repository if it doesn't already exist\n",
    "if \"models\" in pathlib.Path.cwd().parts:\n",
    "    while \"models\" in pathlib.Path.cwd().parts:\n",
    "        os.chdir('..')\n",
    "elif not pathlib.Path('models').exists():\n",
    "    !git clone --depth 1 https://github.com/tensorflow/models\n",
    "\n",
    "!pip install cloud-annotations==0.0.4\n",
    "!pip install tf_slim\n",
    "!pip install --no-deps tensorflowjs==1.4.0\n",
    "\n",
    "%cd models/research\n",
    "!protoc object_detection/protos/*.proto --python_out=.\n",
    "\n",
    "pwd = os.getcwd()\n",
    "sys.path.append(pwd)\n",
    "sys.path.append(pwd + '/slim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-DHE8xTssqT",
    "papermill": {
     "duration": 0.012541,
     "end_time": "2020-10-29T03:18:22.179150",
     "exception": false,
     "start_time": "2020-10-29T03:18:22.166609",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Mount Cloud Annotations Bucket\n",
    "In order to use files from Cloud Annotations we need to mount it to Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-29T03:18:22.200591Z",
     "iopub.status.busy": "2020-10-29T03:18:22.199879Z",
     "iopub.status.idle": "2020-10-29T03:18:22.335927Z",
     "shell.execute_reply": "2020-10-29T03:18:22.335130Z"
    },
    "id": "jydTH9gYQ3y7",
    "papermill": {
     "duration": 0.149513,
     "end_time": "2020-10-29T03:18:22.336315",
     "exception": true,
     "start_time": "2020-10-29T03:18:22.186802",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cloud_annotations'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f01f540af81d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcloud_annotations\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mca\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCLOUD_ANNOTATIONS_MOUNT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cloud_annotations'"
     ]
    }
   ],
   "source": [
    "import cloud_annotations as ca\n",
    "ca.mount(CLOUD_ANNOTATIONS_MOUNT, credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISX8k0TfdDHj",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Generate a Label Map\n",
    "One piece of data the Object Detection API needs is a label map protobuf. The label map associates an integer id to the text representation of the label. The ids are indexed by 1, meaning the first label will have an id of 1 not 0.\n",
    "\n",
    "Here is an example of what a label map looks like:\n",
    "```\n",
    "item {\n",
    "  id: 1\n",
    "  name: 'Cat'\n",
    "}\n",
    "\n",
    "item {\n",
    "  id: 2\n",
    "  name: 'Dog'\n",
    "}\n",
    "\n",
    "item {\n",
    "  id: 3\n",
    "  name: 'Gold Fish'\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJsKCG3UdDsn",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Get a list of labels from the annotations.json\n",
    "labels = {}\n",
    "with open(ANNOTATIONS_JSON_PATH) as f:\n",
    "    annotations = json.load(f)\n",
    "    labels = annotations['labels']\n",
    "\n",
    "# Create a file named label_map.pbtxt\n",
    "os.makedirs(DATA_PATH, exist_ok=True)\n",
    "with open(LABEL_MAP_PATH, 'w') as f:\n",
    "    # Loop through all of the labels and write each label to the file with an id\n",
    "    for idx, label in enumerate(labels):\n",
    "        f.write('item {\\n')\n",
    "        f.write(\"\\tname: '{}'\\n\".format(label))\n",
    "        f.write('\\tid: {}\\n'.format(idx + 1)) # indexes must start at 1\n",
    "        f.write('}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "siRNKiuvsz25",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Generate TFRecords\n",
    "The TensorFlow Object Detection API expects our data to be in the format of TFRecords.\n",
    "\n",
    "The TFRecord format is a collection of serialized feature dicts, one for each image, looking something like this:\n",
    "```\n",
    "{\n",
    "  'image/height': 1800,\n",
    "  'image/width': 2400,\n",
    "  'image/filename': 'image1.jpg',\n",
    "  'image/source_id': 'image1.jpg',\n",
    "  'image/encoded': ACTUAL_ENCODED_IMAGE_DATA_AS_BYTES,\n",
    "  'image/format': 'jpeg',\n",
    "  'image/object/bbox/xmin': [0.7255949630314233, 0.8845598428835489],\n",
    "  'image/object/bbox/xmax': [0.9695875693160814, 1.0000000000000000],\n",
    "  'image/object/bbox/ymin': [0.5820120073891626, 0.1829972290640394],\n",
    "  'image/object/bbox/ymax': [1.0000000000000000, 0.9662484605911330],\n",
    "  'image/object/class/text': (['Cat', 'Dog']),\n",
    "  'image/object/class/label': ([1, 2])\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cAkOvP-gZR1x",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import random\n",
    "\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "\n",
    "from object_detection.utils import dataset_util\n",
    "from object_detection.utils import label_map_util\n",
    "\n",
    "def create_tf_record(images, annotations, label_map, image_path, output):\n",
    "    # Create a train.record TFRecord file.\n",
    "    with tf.python_io.TFRecordWriter(output) as writer:\n",
    "        # Loop through all the training examples.\n",
    "        for image_name in images:\n",
    "            try:\n",
    "                # Make sure the image is actually a file\n",
    "                img_path = os.path.join(image_path, image_name)   \n",
    "                if not os.path.isfile(img_path):\n",
    "                    continue\n",
    "          \n",
    "                # Read in the image.\n",
    "                with tf.gfile.GFile(img_path, 'rb') as fid:\n",
    "                    encoded_jpg = fid.read()\n",
    "\n",
    "                # Open the image with PIL so we can check that it's a jpeg and get the image\n",
    "                # dimensions.\n",
    "                encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "                image = PIL.Image.open(encoded_jpg_io)\n",
    "                if image.format != 'JPEG':\n",
    "                    raise ValueError('Image format not JPEG')\n",
    "\n",
    "                width, height = image.size\n",
    "\n",
    "                # Initialize all the arrays.\n",
    "                xmins = []\n",
    "                xmaxs = []\n",
    "                ymins = []\n",
    "                ymaxs = []\n",
    "                classes_text = []\n",
    "                classes = []\n",
    "\n",
    "                # The class text is the label name and the class is the id. If there are 3\n",
    "                # cats in the image and 1 dog, it may look something like this:\n",
    "                # classes_text = ['Cat', 'Cat', 'Dog', 'Cat']\n",
    "                # classes      = [  1  ,   1  ,   2  ,   1  ]\n",
    "\n",
    "                # For each image, loop through all the annotations and append their values.\n",
    "                for a in annotations[image_name]:\n",
    "                    if (\"x\" in a and \"x2\" in a and \"y\" in a and \"y2\" in a):\n",
    "                        label = a['label']\n",
    "                        xmins.append(a[\"x\"])\n",
    "                        xmaxs.append(a[\"x2\"])\n",
    "                        ymins.append(a[\"y\"])\n",
    "                        ymaxs.append(a[\"y2\"])\n",
    "                        classes_text.append(label.encode(\"utf8\"))\n",
    "                        classes.append(label_map[label])\n",
    "       \n",
    "                # Create the TFExample.\n",
    "                tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                    'image/height': dataset_util.int64_feature(height),\n",
    "                    'image/width': dataset_util.int64_feature(width),\n",
    "                    'image/filename': dataset_util.bytes_feature(image_name.encode('utf8')),\n",
    "                    'image/source_id': dataset_util.bytes_feature(image_name.encode('utf8')),\n",
    "                    'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "                    'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')),\n",
    "                    'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "                    'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "                    'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "                    'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "                    'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "                    'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "                }))\n",
    "                if tf_example:\n",
    "                    # Write the TFExample to the TFRecord.\n",
    "                    writer.write(tf_example.SerializeToString())\n",
    "            except ValueError:\n",
    "                print('Invalid example, ignoring.')\n",
    "                pass\n",
    "            except IOError:\n",
    "                print(\"Can't read example, ignoring.\")\n",
    "                pass\n",
    "\n",
    "with open(ANNOTATIONS_JSON_PATH) as f:\n",
    "    annotations = json.load(f)['annotations']\n",
    "    image_files = [image for image in annotations.keys()]\n",
    "    # Load the label map we created.\n",
    "    label_map = label_map_util.get_label_map_dict(LABEL_MAP_PATH)\n",
    "\n",
    "    random.seed(42)\n",
    "    random.shuffle(image_files)\n",
    "    num_train = int(0.7 * len(image_files))\n",
    "    train_examples = image_files[:num_train]\n",
    "    val_examples = image_files[num_train:]\n",
    "\n",
    "    create_tf_record(train_examples, annotations, label_map, CLOUD_ANNOTATIONS_MOUNT, TRAIN_RECORD_PATH)\n",
    "    create_tf_record(val_examples, annotations, label_map, CLOUD_ANNOTATIONS_MOUNT, VAL_RECORD_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXlvFvwUHrui",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Model config\n",
    "The final thing we need to do is inject our pipline with the amount of labels we have and where to find the label map, TFRecord and model checkpoint. We also need to change the the batch size, because the default batch size of 128 is too large for Colab to handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C8CVExv6HsJS",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from google.protobuf import text_format\n",
    "\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils import label_map_util\n",
    "\n",
    "pipeline_skeleton = '/content/models/research/object_detection/samples/configs/'\n",
    "pipeline_skeleton = pipeline_skeleton + 'ssd_mobilenet_v1_quantized_300x300_coco14_sync.config'\n",
    "configs = config_util.get_configs_from_pipeline_file(pipeline_skeleton)\n",
    "\n",
    "label_map = label_map_util.get_label_map_dict(LABEL_MAP_PATH)\n",
    "num_classes = len(label_map.keys())\n",
    "meta_arch = configs[\"model\"].WhichOneof(\"model\")\n",
    "\n",
    "override_dict = {\n",
    "    'model.{}.num_classes'.format(meta_arch): num_classes,\n",
    "    'train_config.batch_size': 24,\n",
    "    'train_input_path': TRAIN_RECORD_PATH,\n",
    "    'eval_input_path': VAL_RECORD_PATH,\n",
    "    'train_config.fine_tune_checkpoint': 'content/checkpoint/model.ckpt',\n",
    "    'label_map_path': LABEL_MAP_PATH\n",
    "}\n",
    "\n",
    "configs = config_util.merge_external_params_with_configs(configs, kwargs_dict=override_dict)\n",
    "pipeline_config = config_util.create_pipeline_proto_from_configs(configs)\n",
    "config_util.save_pipeline_config(pipeline_config, DATA_PATH)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Copy of object-detection.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2.018856,
   "end_time": "2020-10-29T03:18:22.560024",
   "environment_variables": {},
   "exception": true,
   "input_path": "/Users/niko/Documents/cloud-annotations-elyra/prepare_data.ipynb",
   "output_path": "/Users/niko/Documents/cloud-annotations-elyra/prepare_data.ipynb",
   "parameters": {},
   "start_time": "2020-10-29T03:18:20.541168",
   "version": "2.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}